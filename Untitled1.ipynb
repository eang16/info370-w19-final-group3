{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Student Success in Virtual Learning Environments\n",
    "\n",
    "## Introduction:\n",
    "   The purpose of our research project is to investigate the relationship between student’s use of Virtual Learning Environments (VLE) and their learning performance. We will be using the [Open University Learning Analytics dataset](https://analyse.kmi.open.ac.uk/open_dataset) to study this relationship. Many students enroll in prestigious universities in their pursuit of a higher education, and with the adoption of [e-learning in higher education](http://itdl.org/Journal/Jan_15/Jan15.pdf#page=33), our team was interested in discovering about how well students perform in these virtual environments, and what different factors might influence this learning. Because there is still debate as to whether or not VLE’s guarantee any significant [pedagogical effects](https://telearn.archives-ouvertes.fr/hal-00190701/document), our team is taking a stats driven approach to measure student successes. That being said, our team is not out to compare whether VLE's are more effective than traditional universities. Due to the nature of our dataset, we cannot compare the classes to their traditional counterparts to make any significant deductions. This research is an important undertaking as it would allow us to examine the influence and effectiveness of VLE as an educational, web-based platform for universities and students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis:\n",
    "\n",
    "Trimming the data\n",
    "Before we could begin measuring success, we needed to first determine how to select all the files and features we had from our dataset. Because our data was so massive, we had to make a decision on how to filter out unnecessary data in order to work with a usable size that our scripts would run at a reasonable pace. We started by selecting the `code_module` CCC as our target module to run analysis on. Doing this allowed us to both trim the data down to a more manageable size, and help mitigate inconsistencies between course features. Furthermore, by selecting CCC, we also had to double check that the data was not [imbalanced](https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a).\n",
    "\n",
    "\n",
    "(talk more about combining files, Nan values, and redundant columns?)\n",
    "\n",
    "### age vs final result\n",
    "visual\n",
    "### gender vs final result\n",
    "visual\n",
    "### highest education vs final result\n",
    "visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and combining features\n",
    "\n",
    "We used the data from the `student_info` file as the base to build our student entities. This file included features like `gender`, `region`, and `highest education`. We opted to including these features because we believed that they would help with our [predictions for student success](https://pdfs.semanticscholar.org/e48e/ba98bde33586c20442d46ab9a59c411196e5.pdf). There were multiple assessments that a student could take, each with different weights. We decided that multiplying each assessment by their weights to create weighted scores would provide more insight to the student’s overall quality of their work and speak more to their level of engagement. Beyond the basic student info, we also decided to merge data from the `student_vle` file to add the number of clicks a student had in each module activity. We believed that this would help us further represent a student’s engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a model\n",
    "\n",
    "We knew we wanted to implementMulticlass Classification and Multinomial Logistic Regression. We narrowed down our models between ***Gradient Boosting*** (GBM) and the ***Random Forest*** (RFM) models. Normally, one would also normalize the data to a common scale, but we found that our models were actually less accurate after normalization, so we opted to not include it. And since the Gradient Boosted model had the highest accuracy, we went with a non-normalized GBM for our analysis.\n",
    "\n",
    "Add some markdown formulas here if necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "### GBM Confusion Matrix\n",
    "\n",
    "visual here\n",
    "\n",
    "Using _GBM confusion matrix_ we were able to predict student passing with 84% accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFM Confusion Matrix\n",
    "\n",
    "visual here\n",
    "\n",
    "Using _RFM confusion matrix_ we were able to predict student passing with 82% accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinction vs Non-Distinction RFM\n",
    "\n",
    "visual here\n",
    "\n",
    "We were only 57% accurate in predicting distinction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass vs Fail GBM\n",
    "\n",
    "visual here\n",
    "\n",
    "We were 95% accurate in predicting students passing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
